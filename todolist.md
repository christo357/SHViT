1.  Data-efficiency analysis (core new contribution)   
    
    1. learning curves across datasets: 
        - Seperate for each dataset.
            ‚Ä¢   X-axis: train fraction
            ‚Ä¢	Y-axis: best test_acc1
            ‚Ä¢	Curves: shvit vs deit_tiny

        Report : 
        1. for each model & dataset,  data @ 90% : min. fraction of data where acc.of fracton >=90%. 

        Problems answered:
        1. How many labeled examples each model needs to reach a given accuracy.
        2. Whether SHViT is more data-efficient than DeiT-Tiny in low-data regimes on CIFAR / EuroSAT / MedMNIST.

    2. Area Under Data‚ÄìAccuracy Curve (AUDC)
        For each (model, dataset):
        ‚Ä¢	You have (fraction, best_acc).
        ‚Ä¢	Numerically integrate (trapezoidal) over fractions:

        \text{AUDC = \int_0^{1 \text{Acc}(\text{fraction}) \, d(\text{fraction})

        (approx via discrete sum).

        Interpretation:
            ‚Ä¢	Higher AUDC = better overall data efficiency (strong performance even at low data).
            ‚Ä¢	You can make a table:
            dataset, model , audc



        details:
            It‚Äôs one scalar per (model, dataset) that summarizes the entire learning curve over fractions, not a value ‚Äúfor each fraction‚Äù.

            So:
                ‚Ä¢	X-axis: train fraction (e.g. 0.1, 0.325, 0.55, 0.775, 1.0)
                ‚Ä¢	Y-axis: best test_acc1 achieved at that fraction
                ‚Ä¢	AUDC = area under that curve

            Concretely

                For each dataset D and model M:
                    1.	Collect points:
                (\text{fraction}_i, \text{acc}_i), \quad i=1,\dots,k
                where:
                    ‚Ä¢	fraction_i ‚àà {0.1, 0.325, ‚Ä¶, 1.0}
                    ‚Ä¢	acc_i = best test_acc1 across epochs at that fraction
                    2.	Sort by fraction_i.
                    3.	Compute a discrete integral, e.g. trapezoidal rule:

                AUDC(M, D) \approx \sum_{i=1}^{k-1} [ (acc_i + acc_{i+1})/{2} * (fraction_{i+1} - fraction_i) ]

                That gives you one number:
                    ‚Ä¢	AUDC_shvit_cifar
                    ‚Ä¢	AUDC_deit_cifar
                    ‚Ä¢	AUDC_shvit_eurosat
                    ‚Ä¢	etc.

                You don‚Äôt compute an AUDC ‚Äúfor each fraction‚Äù; the fractions are the x-coordinates you integrate over.



2.  Optimization dynamics & generalization (within training)

    1. Convergence speed (epochs to X% of final accuracy)
        - epochs to reach 95 % acc.
        curve:
        - X-axis: train fraction
	    - Y-axis: Epochs@95%final
	    - Curves: SHViT vs DeiT-Tiny

    2. Generalization gap vs data fraction

        For each epoch:

        GenGap = train loss - testloss

        For each (model, dataset, fraction):
            ‚Ä¢	Compute GenGap at:
            ‚Ä¢	The epoch with best test_acc1, or
            ‚Ä¢	Final epoch.

        Then plot:
            ‚Ä¢	X-axis: train fraction
            ‚Ä¢	Y-axis: GenGap
            ‚Ä¢	Curves: SHViT vs DeiT-Tiny

        What it shows:
            ‚Ä¢	Does one model overfit more at small fractions?
            ‚Ä¢	Is SHViT more ‚Äúregularized‚Äù / DeiT more prone to overfitting in low-data regimes, or vice versa?

        can even form table:
        Dataset, Fraction, Model , Best test_acc1, GenGap at best epoch
        CIFAR,   10%.    , SHViT , ‚Ä¶.             ,‚Ä¶
        CIFAR,   10%.    , Deit-tiny , ‚Ä¶.             ,‚Ä¶


3. Cross-domain comparison of data efficiency (inductive-bias-ish)

        Now combine the above across datasets.

    3.1 ‚ÄúData@X%‚Äù comparisons across domains

        For each model, dataset:
            ‚Ä¢	Compute Data@70%, Data@80%, Data@90% of full-data accuracy.

        Now you can ask:
            ‚Ä¢	On CIFAR (natural-ish images), does SHViT need less data than DeiT to reach 80% of its own max performance?
            ‚Ä¢	On EuroSAT (satellite), maybe SHViT needs more data than DeiT?
            ‚Ä¢	On MedMNIST (medical), maybe both need much more data to reach the same relative performance.

        Table formed:
        Dataset, Model, Full acc, Data@80%, Data@90%
        CIFAR  , SHViT, X%.     , a%      , b%
        CIFAR  , Deit-tiny, X%.     , a%      , b%


        qns answered:
        1. 	If SHViT needs less data on certain domains, that suggests its architecture has a favorable inductive bias there (e.g., maybe for structured textures in EuroSAT).
        2.	If DeiT dominates on another domain, that‚Äôs also interesting.

    
    3.2 Normalized performance across domains

        Sometimes SHViT and DeiT might simply have different capacity. To see inductive bias more cleanly, normalize by each model‚Äôs full-data accuracy:

        RelAcc(fraction) = Acc(fraction) / Acc(100\%)

        Now:
            ‚Ä¢	Plot RelAcc vs fraction, per dataset, for each model.
            ‚Ä¢	This says: ‚ÄúGiven the best this model can do on this dataset, how quickly does it get there?‚Äù

        You can then compare:
            ‚Ä¢	Does SHViT ramp faster (RelAcc is higher at small fractions) on EuroSAT than DeiT?
            ‚Ä¢	Does DeiT ramp faster on MedMNIST?

        That is a very clean way to talk about data efficiency as part of inductive bias, and it‚Äôs not in the SHViT paper




### Test that can be conducted: ( mainly all on cifar)

1( and 2), 3, 7, 

1. Corruption / Noise Robustness (per-domain) ( mainly on cifar)( maybe on eurosat and medmnist for cross domain)

    ‚Ä¢	CIFAR:
        ‚Ä¢	Gaussian noise
        ‚Ä¢	Motion blur
        ‚Ä¢	Brightness/contrast change
        ‚Ä¢	JPEG compression
	‚Ä¢	EuroSAT:
        ‚Ä¢	Gaussian noise (sensor noise)
        ‚Ä¢	Downsample+upsample (low resolution)
        ‚Ä¢	Light fog / haze (brightness + contrast tweak)
	‚Ä¢	MedMNIST:
        ‚Ä¢	Gaussian noise
        ‚Ä¢	Slight rotation
        ‚Ä¢	Contrast change


        1.1 CIFAR ‚Üí ‚ÄúCIFAR-C style‚Äù corruptions

        What: Evaluate each trained model on corrupted versions of CIFAR (you can use CIFAR-10-C/100-C if you‚Äôre on CIFAR-100; or build your own with torchvision.transforms).

        Corruptions to cover (a small but meaningful set):
            ‚Ä¢	Noise: Gaussian, shot, impulse
            ‚Ä¢	Blur: motion blur, defocus blur
            ‚Ä¢	Color/brightness: brightness, contrast
            ‚Ä¢	Digital: JPEG compression, pixelation

        How:

        For each (model, fraction):
            1.	Take the best checkpoint you already saved.
            2.	Build a test loader where you apply one corruption to each image (at a fixed severity).
            3.	Compute test_acc1.
            4.	Repeat for each corruption type and maybe 2‚Äì3 severities.

        Metrics / plots:
            ‚Ä¢	Per-corruption robustness: acc_corr / acc_clean (relative robustness).
            ‚Ä¢	Corruption family mean: average over noise / blur / color / digital.
            ‚Ä¢	Plot: bar plot of robustness per corruption family for SHViT vs DeiT; optionally lines vs fraction (robustness learning curves).

        1.2 EuroSAT ‚Üí domain-specific corruptions

            EuroSAT is satellite imagery, so use ‚Äúremote-sensing-ish‚Äù corruptions:
                ‚Ä¢	Atmospheric noise: Gaussian + speckle noise
                ‚Ä¢	Resolution changes: strong downsampling + upsampling
                ‚Ä¢	Cloud / haze simulation: light fog / brightness shifts (ColorJitter, RandomAffine with low contrast)

            How: same routine as CIFAR:
                ‚Ä¢	For each checkpoint, evaluate on several corrupted test versions.
                ‚Ä¢	Compare robustness vs clean accuracy.

            Question you answer:

            Does SHViT‚Äôs single-head design behave differently from DeiT when spatial resolution or atmospheric effects are degraded?

        1.3 MedMNIST ‚Üí medical-style perturbations

            MedMNIST is grayscale/small medical images. Good corruptions:
                ‚Ä¢	Additive noise (simulating sensor noise).
                ‚Ä¢	Contrast changes (over/under-exposure).
                ‚Ä¢	Small rotations / flips (pose / acquisition changes).
                ‚Ä¢	Random erasing / occlusion (missing tissue or artifacts).

            Same evaluation pattern:
                ‚Ä¢	Accuracy drops vs clean.
                ‚Ä¢	Compare SHViT vs DeiT across corruptions and fractions.

2.  ‚ÄúEfficiency vs Robustness‚Äù Curves (using your fractions)

    This is the nice extension of your existing learning curves.

    For each (dataset, model):
        1.	For each fraction:
        ‚Ä¢	Evaluate the checkpoint on clean test and on corrupted test (choose 1‚Äì2 ‚Äúsummary‚Äù corruptions or an average over several).
        2.	Plot:

        ‚Ä¢	X-axis: train fraction
        ‚Ä¢	Y-axis: accuracy
        ‚Ä¢	Two curves per model: clean vs corrupted

    This tells you:
        ‚Ä¢	How robustness scales with data for each architecture.
        ‚Ä¢	Whether SHViT ‚Äúneeds‚Äù more data than DeiT to become robust, or whether its robustness is more data-efficient.


3. Geometric & Color Invariance Tests( cifar)

    These are easy, no extra dataset needed:

    Build test loaders with only:
        ‚Ä¢	Geometric: rotations (¬±15¬∞, ¬±30¬∞), horizontal flips, small crops & resizes.
        ‚Ä¢	Color: grayscale conversion, heavy color jitter.

    For each (model, fraction):
        ‚Ä¢	Evaluate on:
        ‚Ä¢	clean test
        ‚Ä¢	rotated test
        ‚Ä¢	grayscale test, etc.

    Metrics:
        ‚Ä¢	Invariance score: acc_aug / acc_clean.
        ‚Ä¢	Compare SHViT vs DeiT to see which is more stable to geometry vs color changes on each domain.

4. Calibration & Confidence (from logits)

    From the checkpoints you can also look at how well-calibrated the models are.

    For each (model, dataset, fraction):
        1.	Save softmax probabilities on the clean test set.
        2.	Compute:
        ‚Ä¢	Expected Calibration Error (ECE)
        ‚Ä¢	Brier score
        3.	Make reliability diagrams (predicted prob vs empirical accuracy in bins).

    You can also do this on corrupted test sets:
        ‚Ä¢	Does SHViT become overconfident on corruptions more than DeiT?
        ‚Ä¢	Does calibration degrade more or less with lower train fractions?

    This gives you a ‚Äútrustworthiness‚Äù dimension beyond raw accuracy.

5. Simple OOD Detection Test( cifar )

    You can use your existing models as OOD detectors using just confidence:
        1.	For a model trained on *CIFAR*:
        ‚Ä¢	In-distribution: CIFAR test.
        ‚Ä¢	OOD: something like EuroSAT resized, or noise images, or MedMNIST upsampled.
        2.	Compute:
        ‚Ä¢	Max softmax probability for each sample.
        ‚Ä¢	Plot histograms for ID vs OOD.
        ‚Ä¢	Compute AUROC / AUPR for ‚Äúis this ID?‚Äù based on confidence.

    Repeat for SHViT vs DeiT:
        ‚Ä¢	Which model gives more separation between ID and OOD confidence?
        ‚Ä¢	Does that change with training fraction?

    You can do the same trick swapping ‚Äúdomains‚Äù (EuroSAT as ID, CIFAR/MedMNIST as OOD).

6. Representation Analysis via Probing (lightweight)

    Without retraining from scratch, you can:
        1.	Freeze the backbone (SHViT vs DeiT) trained on, say, full CIFAR fraction.
        2.	Train a linear layer on top for another dataset (e.g., DTD, FGVC-Aircraft, or even EuroSAT/MedMNIST).

    Compare:
        ‚Ä¢	Linear-probe accuracy of SHViT vs DeiT with the same training data.
        ‚Ä¢	That shows which backbone learned more transferable features in each domain.

    You can also compare:
        ‚Ä¢	Linear probe performance when the backbone was trained at 10% vs 100% fraction (data-efficiency of representation learning).

7. Qualitative: Saliency / Attention Maps (for thesis figures)

    Finally, for a couple of nice qualitative plots:
        ‚Ä¢	Use Grad-CAM or attention rollout to visualize:
        ‚Ä¢	Where SHViT looks vs where DeiT looks on the same image (CIFAR, EuroSAT).
        ‚Ä¢	Show:
        ‚Ä¢	An example where SHViT is robust to noise but DeiT fails (and vice versa).
        ‚Ä¢	How the focus changes under corruption.

    This is great for a short ‚Äúinterpretability / inductive bias‚Äù visual subsection.






ood test:
Nice catch‚Äîthat‚Äôs exactly the kind of thing people get stuck on, and the good news is: different class counts are not a problem for OOD tests. üåà

Because in OOD detection, we don‚Äôt care what class the OOD image ‚Äúis‚Äù, we only care how confident the model is that it belongs to some in-distribution class.

‚∏ª

1. Key idea: OOD doesn‚Äôt need matching labels

Take this setup:
	‚Ä¢	Model trained on CIFAR-100 ‚Üí 100 classes (ID domain)
	‚Ä¢	You want to use EuroSAT (10 classes) or MedMNIST (9, 7, etc. classes) as OOD.

When you feed a EuroSAT image into the CIFAR-trained model:
	‚Ä¢	The model still outputs a probability vector of size 100 (over CIFAR classes).
	‚Ä¢	Those class names are wrong for EuroSAT‚Äîbut that‚Äôs fine.
	‚Ä¢	For OOD, we never use the EuroSAT labels at all.

We only use:
	‚Ä¢	ID: CIFAR test images ‚Üí confidences from CIFAR model
	‚Ä¢	OOD: EuroSAT/MedMNIST images ‚Üí confidences from the same CIFAR model

Then we ask:

‚ÄúAre the confidence values different between ID and OOD?‚Äù

So class mismatch doesn‚Äôt matter.

‚∏ª

2. Concrete workflow with different class counts

Let‚Äôs fix:
	‚Ä¢	ID domain = CIFAR (model trained on CIFAR with C_id classes)
	‚Ä¢	OOD domain = EuroSAT (10 classes) or MedMNIST

Step 1 ‚Äì Run the model on ID & OOD

import torch
import torch.nn.functional as F

model.eval()

# --- ID data (CIFAR test) ---
all_logits_id = []
all_labels_id = []  # for sanity / accuracy check (optional)
for x, y in cifar_test_loader:
    x = x.to(device)
    with torch.no_grad():
        logits = model(x)       # shape [N_batch, C_id]
    all_logits_id.append(logits.cpu())
    all_labels_id.append(y)

logits_id = torch.cat(all_logits_id, dim=0)   # [N_id, C_id]
labels_id = torch.cat(all_labels_id, dim=0)   # [N_id]

probs_id = F.softmax(logits_id, dim=1)
conf_id, preds_id = probs_id.max(dim=1)       # [N_id], [N_id]

# --- OOD data (EuroSAT / MedMNIST) ---
all_logits_ood = []
for x, _ in ood_loader:  # ignore OOD labels
    x = x.to(device)
    with torch.no_grad():
        logits = model(x)       # still [N_batch, C_id]
    all_logits_ood.append(logits.cpu())

logits_ood = torch.cat(all_logits_ood, dim=0)  # [N_ood, C_id]
probs_ood = F.softmax(logits_ood, dim=1)
conf_ood, preds_ood = probs_ood.max(dim=1)     # [N_ood], [N_ood]

Notice:
	‚Ä¢	EuroSAT might ‚Äúreally‚Äù have 10 classes, but the model just forces them into 100 CIFAR classes.
	‚Ä¢	We ignore the semantics and only look at conf_ood vs conf_id.

‚∏ª

3. How to turn this into an OOD detection metric

3.1 Confidence histograms (most intuitive)

For each model (SHViT, DeiT):
	‚Ä¢	Plot two histograms:
	‚Ä¢	ID confidences = conf_id
	‚Ä¢	OOD confidences = conf_ood

You want to see:
	‚Ä¢	ID confidences mostly high (e.g. 0.7‚Äì1.0)
	‚Ä¢	OOD confidences lower (e.g. 0.0‚Äì0.4)

Compare SHViT vs DeiT:
	‚Ä¢	If SHViT‚Äôs OOD confidences are lower / more separated, you can say:
‚ÄúSHViT is more cautious on out-of-domain inputs than DeiT-Tiny.‚Äù

3.2 AUROC: single scalar score

Build a binary classification problem: ‚Äúis this sample ID or OOD?‚Äù

import torch
from sklearn.metrics import roc_auc_score

# Scores: higher = more likely ID
scores_id = conf_id        # [N_id]
scores_ood = conf_ood      # [N_ood]

scores = torch.cat([scores_id, scores_ood]).numpy()
labels_bin = torch.cat([
    torch.ones_like(scores_id),     # 1 for ID
    torch.zeros_like(scores_ood)    # 0 for OOD
]).numpy()

auroc = roc_auc_score(labels_bin, scores)
print("AUROC ID vs OOD:", auroc)

	‚Ä¢	AUROC ~1.0 ‚Üí confidence perfectly distinguishes ID vs OOD.
	‚Ä¢	AUROC ~0.5 ‚Üí confidence is useless for OOD (completely overlapping).

Again: no need for class-label compatibility. We never use EuroSAT/MedMNIST labels in this metric.

‚∏ª

4. How to use multiple datasets cleanly

You can repeat the same idea with each dataset as ID in its own experiment:
	1.	ID = CIFAR ‚Üí OOD = EuroSAT, MedMNIST
	2.	ID = EuroSAT ‚Üí OOD = CIFAR, MedMNIST
	3.	ID = MedMNIST ‚Üí OOD = CIFAR, EuroSAT

For each, you have:
	‚Ä¢	Train a model on ID dataset
	‚Ä¢	Use other datasets only as blind OOD inputs
	‚Ä¢	Always use the ID-trained model (class count = #ID classes)
	‚Ä¢	Only look at confidence distributions and AUROC

You never need the labels to match across datasets.

‚∏ª

5. What you don‚Äôt do in OOD

Just to be super clear:
	‚Ä¢	You do not compute ‚Äúaccuracy on EuroSAT using CIFAR labels‚Äù ‚Üí meaningless.
	‚Ä¢	You do not relabel EuroSAT into CIFAR classes.
	‚Ä¢	You do not change the model‚Äôs number of output classes when switching OOD.

You only:
	‚Ä¢	Train model on its own dataset (CIFAR, EuroSAT, or MedMNIST).
	‚Ä¢	At test time, use that trained model to score both ID test and foreign OOD images.
	‚Ä¢	Compare confidence behavior.

‚∏ª

If you want, I can help you pick one clean OOD setup (e.g., ‚ÄúCIFAR as ID, EuroSAT as OOD, full-data SHViT vs DeiT‚Äù) and write a short, self-contained eval_ood.py script around this idea.